# GitHub Issue Agent Demo with AuthBridge (UI Deployment)

This guide walks through deploying the **GitHub Issue Agent** with **AuthBridge**
using the **Kagenti UI** for agent and tool deployment. Infrastructure setup
(webhook, Keycloak, ConfigMaps) is done via CLI, while the agent and tool are
imported and deployed through the Kagenti dashboard.

For a fully manual deployment using only `kubectl`, see [demo-manual.md](demo-manual.md).

This demo extends the [upstream GitHub Issue Agent demo](https://github.com/kagenti/kagenti/blob/main/docs/demos/demo-github-issue.md)
by replacing manual token handling with AuthBridge's automatic token exchange.

## What This Demo Shows

In this demo, we deploy the GitHub Issue Agent and GitHub MCP Tool with AuthBridge
providing end-to-end security:

1. **Agent identity** — The agent automatically registers with Keycloak using its
   SPIFFE ID, with no hardcoded secrets
2. **Inbound validation** — Requests to the agent are validated (JWT signature,
   issuer, and audience) before reaching the agent code
3. **Transparent token exchange** — When the agent calls the GitHub tool, AuthBridge
   automatically exchanges the user's token for one scoped to the tool
4. **Subject preservation** — The end user's identity (`sub` claim) is preserved
   through the exchange, enabling per-user authorization at the tool
5. **Scope-based access** — The tool uses token scopes to determine whether to
   grant public or privileged GitHub API access

## Architecture

```
┌──────────────────────────────────────────────────────────────────────────────────┐
│                              KUBERNETES CLUSTER                                  │
│                                                                                  │
│  ┌───────────────────────────────────────────────────────────────────────────┐   │
│  │                  GIT-ISSUE-AGENT POD (namespace: team1)                   │   │
│  │                                                                           │   │
│  │  ┌─────────────────┐  ┌─────────────┐  ┌──────────────────────────────┐   │   │
│  │  │ git-issue-agent │  │   spiffe-   │  │      client-registration     │   │   │
│  │  │  (A2A agent,    │  │   helper    │  │  (registers with Keycloak    │   │   │
│  │  │   port 8000)    │  │             │  │   using SPIFFE ID)           │   │   │
│  │  └─────────────────┘  └─────────────┘  └──────────────────────────────┘   │   │
│  │                                                                           │   │
│  │  ┌───────────────────────────────────────────────────────────────────┐    │   │
│  │  │                AuthProxy Sidecar (envoy-proxy container)          │    │   │
│  │  │  Envoy + ext_proc (go-processor)                                  │    │   │
│  │  │  Inbound (port 15124):                                            │    │   │
│  │  │    - Validates JWT (signature + issuer + audience via JWKS)       │    │   │
│  │  │    - Returns 401 Unauthorized for invalid/missing tokens          │    │   │
│  │  │  Outbound (port 15123):                                           │    │   │
│  │  │    - HTTP: Exchanges token via Keycloak → aud: github-tool        │    │   │
│  │  │    - HTTPS: TLS passthrough (no interception)                     │    │   │
│  │  └───────────────────────────────────────────────────────────────────┘    │   │
│  └───────────────────────────────────────────────────────────────────────────┘   │
│                                      │                                           │
│                      Exchanged token │(aud: github-tool)                         │
│                                      ▼                                           │
│  ┌───────────────────────────────────────────────────────────────────────────┐   │
│  │                  GITHUB-TOOL POD (namespace: team1)                       │   │
│  │                                                                           │   │
│  │  ┌──────────────────────────────────────────────────────────────────┐     │   │
│  │  │                     github-tool (port 9090)                      │     │   │
│  │  │  - Validates token (aud: github-tool, issuer: Keycloak)          │     │   │
│  │  │  - Token has github-full-access scope? → PRIVILEGED_ACCESS_PAT   │     │   │
│  │  │  - Otherwise → PUBLIC_ACCESS_PAT                                 │     │   │
│  │  └──────────────────────────────────────────────────────────────────┘     │   │
│  └───────────────────────────────────────────────────────────────────────────┘   │
│                                                                                  │
├──────────────────────────────────────────────────────────────────────────────────┤
│                            EXTERNAL SERVICES                                     │
│                                                                                  │
│  ┌──────────────────────┐          ┌──────────────────────┐                      │
│  │   SPIRE (namespace:  │          │ KEYCLOAK (namespace: │                      │
│  │       spire)         │          │     keycloak)        │                      │
│  │                      │          │                      │                      │
│  │  Provides SPIFFE     │          │  - demo realm        │                      │
│  │  identities (SVIDs)  │          │  - token exchange    │                      │
│  └──────────────────────┘          └──────────────────────┘                      │
└──────────────────────────────────────────────────────────────────────────────────┘
```

## Key Security Properties

| Property | How It's Achieved |
|----------|-------------------|
| **No hardcoded agent secrets** | Client credentials dynamically generated by client-registration using SPIFFE ID |
| **Identity-based auth** | SPIFFE ID is both the pod identity and the Keycloak client ID |
| **Inbound validation** | [AuthProxy](../../AuthProxy/README.md) validates all incoming requests (JWT signature, issuer, audience) before they reach the agent |
| **Audience-scoped tokens** | Original token scoped to Agent; exchanged token scoped to GitHub tool |
| **User attribution** | `sub` and `preferred_username` preserved through token exchange |
| **Scope-based authorization** | Tool uses token scopes to determine access level (public vs. privileged) |
| **Transparent to agent code** | The agent makes plain HTTP calls; AuthBridge handles all token management |

### Inbound Verification (AuthProxy)

The AuthBridge sidecar includes [AuthProxy](../../AuthProxy/README.md), an Envoy-based
ext_proc that validates **every** inbound request before it reaches the agent. The
ext_proc (port 9090) performs three checks on the `Authorization: Bearer <token>` header:

1. **Signature** — Verifies the JWT signature against Keycloak's JWKS keys
   (auto-refreshed via cache). Rejects tampered or forged tokens.
2. **Issuer** — Confirms the `iss` claim matches the expected Keycloak realm
   (`ISSUER` in `authbridge-config`). Rejects tokens from other identity providers.
3. **Audience** — If `EXPECTED_AUDIENCE` is set, confirms the `aud` claim includes
   the agent's SPIFFE ID. Rejects tokens intended for a different service.

Requests that fail any check receive an immediate `401 Unauthorized` response from
Envoy — the agent application never sees them. This is tested in
[Step 9b–9c](#step-9-test-via-cli-optional).

---

## Prerequisites

Ensure you have completed the Kagenti platform setup as described in the
[Installation Guide](https://github.com/kagenti/kagenti/blob/main/docs/install.md),
including the Kagenti UI.

You should also have:
- The [kagenti-extensions](https://github.com/kagenti/kagenti-extensions) repo cloned
- The Kagenti UI running at `http://kagenti-ui.localtest.me:8080`
- Python 3.9+ with `venv` support
- **Ollama running** with the `ibm/granite4:latest` model (or another model of your choice)
- Two GitHub Personal Access Tokens (PATs):
  - `<PUBLIC_ACCESS_PAT>` — access to public repositories only
  - `<PRIVILEGED_ACCESS_PAT>` — access to all repositories

See the [upstream demo](https://github.com/kagenti/kagenti/blob/main/docs/demos/demo-github-issue.md#required-github-pat-tokens) for instructions on creating GitHub PAT tokens.

---

## Step 1: Configure Keycloak

Keycloak needs to be configured with the correct clients, scopes, and users for the
token exchange flow between the agent and the GitHub tool.

### Port-forward Keycloak (if needed)

The setup script connects to Keycloak at `http://keycloak.localtest.me:8080`.
If Keycloak is not already reachable at that address (e.g., via an ingress),
start a port-forward in a separate terminal:

```bash
kubectl port-forward service/keycloak-service -n keycloak 8080:8080
```

### Run the setup script

```bash
cd AuthBridge

# Create virtual environment (if not already done)
python -m venv venv
source venv/bin/activate
pip install --upgrade pip
pip install -r requirements.txt

# Run the Keycloak setup for this demo
python demos/github-issue/setup_keycloak.py
```

This creates:

| Resource | Name | Purpose |
|----------|------|---------|
| **Realm** | `demo` | Keycloak realm for the demo |
| **Client** | `github-tool` | Target audience for token exchange |
| **Scope** | `agent-team1-git-issue-agent-aud` | Realm DEFAULT — auto-adds Agent's SPIFFE ID to all tokens |
| **Scope** | `github-tool-aud` | Realm OPTIONAL — for exchanged tokens targeting the tool |
| **Scope** | `github-full-access` | Realm OPTIONAL — for privileged GitHub API access |
| **User** | `alice` (password: `alice123`) | Regular user — public access |
| **User** | `bob` (password: `bob123`) | Privileged user — full access |

---

## Step 2: Apply ConfigMaps

> **Critical: Apply ConfigMaps BEFORE importing the agent in the UI.** The agent pod
> reads ConfigMap values at startup. If the agent starts before the ConfigMaps are
> correct, the client will be registered in the wrong Keycloak realm.

```bash
cd AuthBridge

# Apply the GitHub Issue demo ConfigMaps
kubectl apply -f demos/github-issue/k8s/configmaps.yaml
```

Verify:

```bash
kubectl get configmap environments -n team1 -o jsonpath='{.data.KEYCLOAK_REALM}'
# Expected: demo
```

---

## Step 3: Create the GitHub Tool Secrets

The GitHub tool needs PAT tokens to access the GitHub API. Create a Kubernetes secret
with your tokens before importing the tool:

```bash
export PRIVILEGED_ACCESS_PAT=<your-privileged-pat>
export PUBLIC_ACCESS_PAT=<your-public-pat>
```

Provide your actual GitHub Personal Access Tokens.

```bash
kubectl create secret generic github-tool-secrets -n team1 \
  --from-literal=INIT_AUTH_HEADER="Bearer $PRIVILEGED_ACCESS_PAT" \
  --from-literal=UPSTREAM_HEADER_TO_USE_IF_IN_AUDIENCE="Bearer $PRIVILEGED_ACCESS_PAT" \
  --from-literal=UPSTREAM_HEADER_TO_USE_IF_NOT_IN_AUDIENCE="Bearer $PUBLIC_ACCESS_PAT"
```

---

## Step 4: Import the GitHub Tool via Kagenti UI

1. Navigate to [Import Tool](http://kagenti-ui.localtest.me:8080/tools/import)
   in the Kagenti UI.

2. In the **Namespace** drop-down, choose `team1`.

3. Select **Build from Source** as the deployment method.

4. Under **Source Code** select:
   - **Git Repository URL**: `https://github.com/kagenti/agent-examples`
   - **Branch or Tag**: `main`
   - **Example Tools**: `GitHub Tool`
   - **Source Subfolder**: `mcp/github_tool`

5. **Workload Type** select `Deployment`

6. Set **MCP Transport Protocol** to `streamable HTTP`

7. Under **Port Configuration**, set **Service Port** to `9090` and **Target Port** to `9090`

   > The tool binary listens on port 9090. The agent's `MCP_URL` connects to
   > `http://github-tool-mcp:9090/mcp`, so both the service port and target port
   > must be 9090 to match.

8. Under **Environment Variables**, click **Import from File/URL**,
   Select **From URL** and provide the `.env` file from this repo:
   - **URL** `https://raw.githubusercontent.com/kagenti/agent-examples/refs/heads/main/mcp/github_tool/.env.authbridge`
   - Click **Fetch & Parse** — this populates all environment variables, including
     Secret references for the PAT tokens and direct values for Keycloak settings.
   - Click **Import** to set all the env. variables.

   The imported variables will show three **Secret** type entries referencing
   `github-tool-secrets` and three **Direct Value** entries for Keycloak configuration.
   No manual editing is needed.

   > **Tip:** You can also upload the file directly from your local system.

9. Click **Build & Deploy New Tool**.

You will be redirected to a **Build Progress** page where you can monitor the
Shipwright build. Wait for it to complete.

> **Note:** If the build fails with `short-name resolution enforced`, the Dockerfiles
> in the `agent-examples` repo need fully qualified image names. This was fixed in
> [kagenti/agent-examples#125](https://github.com/kagenti/agent-examples/pull/125)
> (merged). If you're on an older branch, rebase onto `main`.

### Patch: Remove AuthBridge sidecars from the tool

<!-- WORKAROUND: Remove this entire patch section once kagenti-extensions#138 is fixed. -->

> **Known issue ([kagenti-extensions#138](https://github.com/kagenti/kagenti-extensions/issues/138)):**
> The webhook injects AuthBridge sidecars into the tool deployment even though the
> tool has `kagenti.io/inject: disabled`. This happens because `InjectAuthBridge`
> uses the `PrecedenceEvaluator`, which only checks per-sidecar labels
> (`kagenti.io/envoy-proxy-inject`, etc.) — not the master `kagenti.io/inject`
> label. The injected go-processor (ext_proc) gRPC server and the GitHub tool's
> MCP broker both bind to port 9090, causing a port conflict that makes the tool
> unreachable (agent gets "Couldn't connect to the MCP server").

Remove the sidecars by temporarily disabling the webhook, redeploying clean, then
restoring:

```bash
# 1. Disable the webhook
kubectl scale deployment kagenti-webhook-controller-manager \
  -n kagenti-webhook-system --replicas=0
kubectl patch mutatingwebhookconfiguration \
  kagenti-webhook-authbridge-mutating-webhook-configuration \
  --type='json' -p='[{"op":"replace","path":"/webhooks/0/failurePolicy","value":"Ignore"}]'

# 2. Delete and recreate the tool without sidecars
kubectl get deployment github-tool -n team1 -o json | python3 -c "
import sys, json
d = json.load(sys.stdin)
spec = d['spec']['template']['spec']
spec['containers'] = [c for c in spec['containers'] if c['name'] == 'mcp']
spec.pop('initContainers', None)
injected = {'envoy-config','shared-data','svid-output','spire-agent-socket','spiffe-helper-config','authbridge-config'}
spec['volumes'] = [v for v in spec.get('volumes',[]) if v['name'] not in injected]
if not spec.get('volumes'): spec.pop('volumes', None)
for c in spec['containers']:
    if 'volumeMounts' in c:
        c['volumeMounts'] = [vm for vm in c['volumeMounts'] if vm['name'] not in injected]
        if not c['volumeMounts']: del c['volumeMounts']
labels = d['spec']['template']['metadata']['labels']
labels['kagenti.io/inject'] = 'disabled'
labels['kagenti.io/envoy-proxy-inject'] = 'false'
labels['kagenti.io/client-registration-inject'] = 'false'
for k in ['resourceVersion','uid','creationTimestamp','generation','managedFields']:
    d['metadata'].pop(k, None)
d.pop('status', None)
d['metadata'].pop('annotations', None)
d['spec']['template']['metadata'].pop('annotations', None)
json.dump(d, sys.stdout)
" > /tmp/github-tool-clean.json

kubectl delete deployment github-tool -n team1
sleep 2
kubectl create -f /tmp/github-tool-clean.json

# 3. Wait for clean rollout
kubectl rollout status deployment/github-tool -n team1 --timeout=60s

# 4. Restore the webhook
kubectl scale deployment kagenti-webhook-controller-manager \
  -n kagenti-webhook-system --replicas=1
kubectl patch mutatingwebhookconfiguration \
  kagenti-webhook-authbridge-mutating-webhook-configuration \
  --type='json' -p='[{"op":"replace","path":"/webhooks/0/failurePolicy","value":"Fail"}]'
```

> **Important:** The script adds **per-sidecar opt-out labels**
> (`kagenti.io/envoy-proxy-inject: "false"`, `kagenti.io/client-registration-inject: "false"`)
> in addition to `kagenti.io/inject: disabled`. These per-sidecar labels are what the
> `PrecedenceEvaluator` actually checks, so they prevent re-injection on subsequent
> rollouts even with the webhook active.
>
> **Do not use `kubectl apply`** to update the deployment — Kubernetes strategic merge
> patches merge the `containers` list by name rather than replacing it, so removed
> containers reappear. Use `kubectl delete` + `kubectl create` (as above) or
> `kubectl replace --force`.

Verify the tool is running with only 1 container:

```bash
kubectl get pods -n team1 | grep github-tool
# Expected: github-tool-xxxxx   1/1   Running   0   ...
```

### Verify the tool is reachable

Confirm the tool service port is correct and the tool responds:

```bash
kubectl run test-mcp --image=curlimages/curl -n team1 --restart=Never --rm -it -- \
  curl -s -o /dev/null -w "%{http_code}" --max-time 5 http://github-tool-mcp:9090/mcp
# Expected: 200 (SSE connection, may timeout after 5s — that's OK)
```

---

## Step 5: Import the GitHub Issue Agent via Kagenti UI

1. Navigate to [Import Agent](http://kagenti-ui.localtest.me:8080/agents/import)
   in the Kagenti UI.

2. In the **Namespace** drop-down, choose `team1`.

3. Select **Build from Source** as the deployment method.

4. Under **Source Repository** select:
   - **Git Repository URL**: `https://github.com/kagenti/agent-examples`
   - **Git Branch**: `main`
   - **Select Example**: `Git Issue Agent`
   - **Source Path**: `a2a/git_issue_agent`

5. **Protocol**: `A2A`

6. **Framework**: `LangGraph`

7. **Workload Type** select `Deployment`.

8. Make sure **Enable AuthBridge sidecar injection** is checked.

9. Make sure **Enable SPIRE identity (spiffe-helper sidecar)** is checked.

10. Under **Port Configuration**, set **Service Port** to `8080` and **Target Port** to `8000`

11. Under **Environment Variables**, click **Import from File/URL**,
   Select **From URL** and provide the **URL** from this repo:
    - For Ollama: `https://raw.githubusercontent.com/kagenti/agent-examples/refs/heads/main/a2a/git_issue_agent/.env.ollama`
    - For OpenAI: `https://raw.githubusercontent.com/kagenti/agent-examples/refs/heads/main/a2a/git_issue_agent/.env.openai`
    - Click **Fetch & Parse** — this populates all environment variables including
     LLM settings, `MCP_URL`, and `JWKS_URI`. No manual editing is needed.
    - Click **Import** to set all the env. variables.

   The Ollama variant sets all direct values. The OpenAI variant includes
   **Secret** type entries referencing `openai-secret` for `LLM_API_KEY`
   and `OPENAI_API_KEY`.

   > **Tip:** You can also upload the file directly from your local system.
   > **OpenAI prerequisite:** If using OpenAI, create the secret first:
   > ```bash
   > kubectl create secret generic openai-secret -n team1 \
   >   --from-literal=apikey="<YOUR_OPENAI_API_KEY>"
   > ```

12. Click **Build & Deploy Agent**.

Wait for the Shipwright build to complete and the deployment to become ready.

### Patch: Enable SPIRE identity

<!-- WORKAROUND: Remove this entire "Patch: Enable SPIRE identity" section
     (through "Wait for the rollout") once kagenti/kagenti#738 is fixed. -->

> **Known issue:** The Kagenti UI drops the `kagenti.io/spire: enabled` label on the
> final deployment pass ([kagenti/kagenti#738](https://github.com/kagenti/kagenti/issues/738)),
> causing the webhook to inject sidecars without SPIRE support. Until this is fixed,
> apply the following patch after the UI deploys the agent.

**1. Create the service account** (SPIRE identity is derived from the pod's SA):

```bash
kubectl create sa git-issue-agent -n team1
```

**2. Patch the deployment** to add spiffe-helper, SPIRE volumes, and the SPIRE-aware
client-registration script:

```bash
kubectl patch deployment git-issue-agent -n team1 --type=json -p '[
  {"op":"add","path":"/spec/template/spec/serviceAccountName","value":"git-issue-agent"},
  {"op":"replace","path":"/spec/template/metadata/labels/kagenti.io~1spire","value":"enabled"},
  {"op":"replace","path":"/spec/template/spec/containers/1/env/0/value","value":"true"},
  {"op":"replace","path":"/spec/template/spec/containers/1/command","value":["/bin/sh","-c","\necho \"Waiting for SPIFFE credentials...\"\nwhile [ ! -f /opt/jwt_svid.token ]; do\n  echo \"waiting for SVID\"\n  sleep 1\ndone\necho \"SPIFFE credentials ready!\"\nJWT_PAYLOAD=$(cat /opt/jwt_svid.token | cut -d\".\" -f2)\nif ! CLIENT_ID=$(echo \"${JWT_PAYLOAD}==\" | base64 -d | python -c \"import sys,json; print(json.load(sys.stdin).get(\\\"sub\\\",\\\"\\\"))\"); then\n  echo \"Error: Failed to decode JWT payload or extract client ID\" >&2\n  exit 1\nfi\nif [ -z \"$CLIENT_ID\" ]; then\n  echo \"Error: Extracted client ID is empty\" >&2\n  exit 1\nfi\necho \"$CLIENT_ID\" > /shared/client-id.txt\necho \"Client ID (SPIFFE ID): $CLIENT_ID\"\necho \"Starting client registration...\"\npython client_registration.py\necho \"Client registration complete!\"\ntail -f /dev/null\n"]},
  {"op":"add","path":"/spec/template/spec/containers/1/volumeMounts/-","value":{"name":"svid-output","mountPath":"/opt"}},
  {"op":"add","path":"/spec/template/spec/volumes/-","value":{"name":"spire-agent-socket","csi":{"driver":"csi.spiffe.io","readOnly":true}}},
  {"op":"add","path":"/spec/template/spec/volumes/-","value":{"name":"spiffe-helper-config","configMap":{"name":"spiffe-helper-config"}}},
  {"op":"add","path":"/spec/template/spec/volumes/-","value":{"name":"svid-output","emptyDir":{}}},
  {"op":"add","path":"/spec/template/spec/containers/-","value":{"name":"spiffe-helper","image":"ghcr.io/spiffe/spiffe-helper:nightly","command":["/spiffe-helper","-config=/etc/spiffe-helper/helper.conf","run"],"securityContext":{"runAsUser":1000,"runAsGroup":1000},"volumeMounts":[{"name":"spiffe-helper-config","mountPath":"/etc/spiffe-helper"},{"name":"spire-agent-socket","mountPath":"/spiffe-workload-api"},{"name":"svid-output","mountPath":"/opt"},{"name":"shared-data","mountPath":"/shared"}]}}
]'
```

**3. Wait for the rollout:**

```bash
kubectl rollout status deployment/git-issue-agent -n team1 --timeout=120s
```

---

## Step 6: Verify the Deployment

### Check pod status

```bash
kubectl get pods -n team1
```

Expected output:

```
NAME                               READY   STATUS    RESTARTS   AGE
git-issue-agent-58768bdb67-xxxxx   4/4     Running   0          2m
github-tool-7f8c9d6b44-yyyyy      1/1     Running   0          5m
```

> **Note:** The agent pod should show **4/4** containers — the agent itself plus
> three AuthBridge sidecars (spiffe-helper, kagenti-client-registration, envoy-proxy)
> injected by the webhook.

### Verify injected containers

```bash
kubectl get pod -n team1 -l app.kubernetes.io/name=git-issue-agent -o jsonpath='{.items[0].spec.containers[*].name}'
```

Expected (after applying the SPIRE patch above):

```
agent kagenti-client-registration envoy-proxy spiffe-helper
```

If the SPIRE patch was not applied, you will see only 3 containers:

```
agent kagenti-client-registration envoy-proxy
```

> **Note:** Both the UI and manual deployments use the same naming conventions:
> container name `agent`, labels `app.kubernetes.io/name: git-issue-agent`,
> and service `git-issue-agent:8080`.

### Check client registration

```bash
kubectl logs deployment/git-issue-agent -n team1 -c kagenti-client-registration
```

Expected:

```
SPIFFE credentials ready!
Client ID (SPIFFE ID): spiffe://localtest.me/ns/team1/sa/git-issue-agent
Created Keycloak client "spiffe://localtest.me/ns/team1/sa/git-issue-agent"
Client registration complete!
```

### Check agent logs

```bash
kubectl logs deployment/git-issue-agent -n team1 -c agent
```

Expected:

```
SVID JWT file /opt/jwt_svid.token not found.
SVID JWT file /opt/jwt_svid.token not found.
CLIENT_SECRET file not found at /shared/secret.txt
INFO: JWKS_URI is set - using JWT Validation middleware
INFO:     Started server process [17]
INFO:     Waiting for application startup.
INFO:     Application startup complete.
INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)
```

<!-- WORKAROUND: Remove this warning note once kagenti/agent-examples#129 is fixed. -->

> **These warnings are expected and harmless.** The agent's built-in auth code
> probes for SVID and client-secret files at startup. With AuthBridge, these files
> are used by the sidecars (spiffe-helper, client-registration, Envoy), not by the
> agent container directly. The agent falls back to JWKS-based JWT validation
> (`JWKS_URI is set`), which is the correct behavior — AuthBridge's Envoy sidecar
> handles inbound JWT validation and outbound token exchange on behalf of the agent.
> These warnings will be removed once the agent's built-in auth logic is cleaned up
> ([kagenti/agent-examples#129](https://github.com/kagenti/agent-examples/issues/129)).

### Check the service endpoint

```bash
kubectl get svc -n team1 | grep git-issue-agent
```

Expected:

```
git-issue-agent   ClusterIP   10.96.x.x   <none>   8080/TCP   5m
```

The service maps **port 8080** to the agent's internal port 8000 (same for both
UI and manual deployments).

---

## Step 7: Verify Ollama is Running

The agent uses an LLM for inference. If using Ollama, verify it is running:

```bash
ollama list
```

You should see `ibm/granite4:latest` (or whichever model you configured) on the list.
If Ollama is not running, start it in a separate terminal (`ollama serve`) and ensure the
model is pulled (`ollama pull ibm/granite4:latest`).

---

## Step 8: Chat via Kagenti UI

<!-- WORKAROUND: Remove this limitation note once the Kagenti UI uses the demo realm
     for agent communication, or once go-processor supports multiple JWKS URLs.
     Track: https://github.com/kagenti/kagenti-extensions/issues — no issue filed yet. -->

> **Known limitation:** The Kagenti UI authenticates via Keycloak's `master` realm,
> but AuthBridge validates inbound JWTs against the `demo` realm JWKS. Since the
> go-processor currently supports only a single JWKS URL, UI chat requests receive
> a `401` ("key not found in key set"). The **Agent Card** page works because
> `/.well-known/*` paths bypass JWT validation
> ([PR #133](https://github.com/kagenti/kagenti-extensions/pull/133)).
>
> **Use [Step 9: Test via CLI](#step-9-test-via-cli-optional)** to test the full
> AuthBridge flow end-to-end with a valid `demo`-realm token.

1. Navigate to the **Agent Catalog** in the Kagenti UI.
2. Select the `team1` namespace.
3. Under **Available Agents**, select `git-issue-agent` and click **View Details**.
4. Verify the **Agent Card** is visible (this confirms the agent is running and
   the `/.well-known/*` bypass is working).
5. Chat via the UI will not work until the realm mismatch is resolved (see note above).
   Use the CLI test in Step 9 instead.

---

## Step 9: Test via CLI

Test the AuthBridge flow from the command line to verify inbound validation and
token exchange using a `demo`-realm token.

> **Note:** The CLI test commands below use the same service name and port
> (`git-issue-agent:8080`) as both the UI and manual deployments.

### Setup

```bash
# Start a test client pod
kubectl run test-client --image=nicolaka/netshoot -n team1 --restart=Never -- sleep 3600
kubectl wait --for=condition=ready pod/test-client -n team1 --timeout=30s
```

### 9a. Agent Card - Public Endpoint (No Token Required)

The `/.well-known/agent.json` endpoint is publicly accessible — AuthBridge's
go-processor [bypasses JWT validation](https://github.com/kagenti/kagenti-extensions/pull/133)
for `/.well-known/*`, `/healthz`, `/readyz`, and `/livez` by default:

```bash
kubectl exec test-client -n team1 -- curl -s \
  http://git-issue-agent:8080/.well-known/agent.json | jq .name
# Expected: "Github issue agent"
```

### 9b. Inbound Rejection - No Token

Non-public endpoints require a valid JWT:

```bash
kubectl exec test-client -n team1 -- curl -s \
  http://git-issue-agent:8080/
# Expected: {"error":"unauthorized","message":"missing Authorization header"}
```

### 9c. Inbound Rejection - Invalid Token (Signature Check)

A malformed or tampered token fails the JWKS signature check:

```bash
kubectl exec test-client -n team1 -- curl -s \
  -H "Authorization: Bearer invalid-token" \
  http://git-issue-agent:8080/
# Expected: {"error":"unauthorized","message":"token validation failed: failed to parse/validate token: ..."}
```

### 9d. End-to-End Test with Valid Token

Open a shell inside the test-client pod to avoid JWT shell expansion issues:

```bash
kubectl exec -it test-client -n team1 -- sh
```

Inside the pod, get credentials and send a request:

```bash
# Get a Keycloak admin token
ADMIN_TOKEN=$(curl -s http://keycloak-service.keycloak.svc:8080/realms/master/protocol/openid-connect/token \
  -d "grant_type=password" \
  -d "client_id=admin-cli" \
  -d "username=admin" \
  -d "password=admin" | jq -r ".access_token")

echo "Admin token length: ${#ADMIN_TOKEN}"

# Look up the agent's client in the demo realm.
# The client ID is the SPIFFE ID (URL-encoded in the query parameter).
SPIFFE_ID="spiffe://localtest.me/ns/team1/sa/git-issue-agent"
CLIENTS=$(curl -s -H "Authorization: Bearer $ADMIN_TOKEN" \
  "http://keycloak-service.keycloak.svc:8080/admin/realms/demo/clients" \
  --data-urlencode "clientId=$SPIFFE_ID" --get)
INTERNAL_ID=$(echo "$CLIENTS" | jq -r ".[0].id")
CLIENT_ID=$(echo "$CLIENTS" | jq -r ".[0].clientId")

echo "Internal ID:   $INTERNAL_ID"
echo "Client ID:     $CLIENT_ID"

# Get the client secret (extract directly from the client listing;
# the /client-secret endpoint may return null for auto-registered clients)
CLIENT_SECRET=$(echo "$CLIENTS" | jq -r ".[0].secret")

echo "Secret length: ${#CLIENT_SECRET}"

# Get an OAuth token for the agent
TOKEN=$(curl -s -X POST \
  "http://keycloak-service.keycloak.svc:8080/realms/demo/protocol/openid-connect/token" \
  -d "grant_type=client_credentials" \
  --data-urlencode "client_id=$CLIENT_ID" \
  --data-urlencode "client_secret=$CLIENT_SECRET" | jq -r ".access_token")

echo "Token length:  ${#TOKEN}"

# Send a prompt to the agent (A2A v0.3.0)
curl -s --max-time 300 \
  -H "Authorization: Bearer $TOKEN" \
  -H "Content-Type: application/json" \
  -X POST http://git-issue-agent:8080/ \
  -d '{
    "jsonrpc": "2.0",
    "id": "test-1",
    "method": "message/send",
    "params": {
      "message": {
        "role": "user",
        "messageId": "msg-001",
        "parts": [{"type": "text", "text": "List issues in kagenti/kagenti repo"}]
      }
    }
  }' | jq
```

Exit the pod when done:

```bash
exit
```

### 9e. Verify AuthProxy Logs (Inbound + Outbound)

Check the ext_proc logs to confirm both inbound validation and outbound token
exchange are working:

**Inbound validation logs:**

```bash
kubectl logs deployment/git-issue-agent -n team1 -c envoy-proxy 2>&1 | grep "\[Inbound\]"
```

Expected:

```
[Inbound] Token validated - issuer: http://keycloak.localtest.me:8080/realms/demo, audience: [spiffe://localtest.me/ns/team1/sa/git-issue-agent ...]
[Inbound] JWT validation succeeded, forwarding request
```

**Outbound token exchange logs:**

```bash
kubectl logs deployment/git-issue-agent -n team1 -c envoy-proxy 2>&1 | grep "^2026/" | grep "\[Token Exchange\]"
```

Expected:

```
[Token Exchange] Token URL: http://keycloak-service.keycloak.svc:8080/realms/demo/protocol/openid-connect/token
[Token Exchange] Client ID: spiffe://localtest.me/ns/team1/sa/git-issue-agent
[Token Exchange] Audience: github-tool
[Token Exchange] Scopes: openid github-tool-aud github-full-access
[Token Exchange] Successfully exchanged token
[Token Exchange] Successfully exchanged token, replacing Authorization header
```

### Clean Up Test Client

```bash
kubectl delete pod test-client -n team1 --ignore-not-found
```

---

## Patching Agent Environment (If Needed)

If the agent is missing environment variables after UI deployment (e.g., `MCP_URL`,
`JWKS_URI`, or LLM keys), you can patch the deployment:

```bash
# Set missing env vars on the agent container
kubectl set env deployment/git-issue-agent -n team1 -c agent \
  MCP_URL="http://github-tool-mcp:9090/mcp" \
  JWKS_URI="http://keycloak-service.keycloak.svc:8080/realms/demo/protocol/openid-connect/certs"

# If using OpenAI and the key is in a secret:
kubectl patch deployment git-issue-agent -n team1 --type=json -p='[
  {"op":"add","path":"/spec/template/spec/containers/0/env/-","value":{
    "name":"LLM_API_KEY",
    "valueFrom":{"secretKeyRef":{"name":"openai-secret","key":"apikey"}}
  }},
  {"op":"add","path":"/spec/template/spec/containers/0/env/-","value":{
    "name":"OPENAI_API_KEY",
    "valueFrom":{"secretKeyRef":{"name":"openai-secret","key":"apikey"}}
  }}
]'

# Wait for rollout
kubectl rollout status deployment/git-issue-agent -n team1 --timeout=180s
```

---

## How AuthBridge Changes the Original Demo

| Aspect | Original Demo | With AuthBridge |
|--------|--------------|-----------------|
| **Agent secrets** | Manual PAT token configuration | Dynamic credentials via SPIFFE + client-registration |
| **Inbound auth** | No validation | [AuthProxy](../../AuthProxy/README.md) validates JWT (signature, issuer, audience) via ext_proc |
| **Token management** | Agent code handles tokens | Transparent sidecar — agent code unchanged |
| **Token for tool** | Same PAT token passed through | OAuth token exchange (RFC 8693) |
| **User attribution** | No user tracking | `sub` claim preserved through exchange |
| **Access control** | Single PAT for all users | Scope-based: public vs. privileged |

---

## Troubleshooting

### Invalid Client or Invalid Client Credentials

**Symptom:** `{"error":"invalid_client","error_description":"Invalid client or Invalid client credentials"}`

**Cause:** The client was registered in the wrong Keycloak realm (typically `master`
instead of `demo`). This happens when the `environments` ConfigMap is updated **after**
the agent pod has already started. This is especially common with UI deployments where
the Kagenti Helm chart may have pre-existing ConfigMaps.

**Fix:**

```bash
# 1. Verify ConfigMap is correct
kubectl get configmap environments -n team1 -o jsonpath='{.data.KEYCLOAK_REALM}'
# Should show: demo

# 2. If wrong, re-apply ConfigMaps and restart
kubectl apply -f demos/github-issue/k8s/configmaps.yaml
kubectl rollout restart deployment/git-issue-agent -n team1
```

### Build Fails: "short-name resolution enforced"

<!-- WORKAROUND: Remove this section once all agent-examples Dockerfiles use FQDN images.
     Fixed in kagenti/agent-examples#125 (merged). -->

**Symptom:** Shipwright build fails with `Error: creating build container: short-name resolution enforced but cannot prompt without a TTY`

**Cause:** Podman/Buildah requires fully qualified image names (e.g., `docker.io/library/golang:...`)
but the Dockerfiles use short names.

**Fix:** This was fixed in [kagenti/agent-examples#125](https://github.com/kagenti/agent-examples/pull/125)
(merged). If you're on an older branch, rebase onto `main`.

### Agent Missing Environment Variables

**Symptom:** Agent returns `JWKS_URI or GITHUB_TOKEN env var must be set` or similar

**Cause:** The UI deployment didn't include all required environment variables.

**Fix:** See the [Patching Agent Environment](#patching-agent-environment-if-needed) section above.

### Service Name Mismatch

**Symptom:** `Couldn't resolve host` when trying to reach the agent

**Fix:** Verify the service exists and check the name/port:

```bash
kubectl get svc -n team1 | grep git-issue-agent
```

Both the UI and manual deployments create `git-issue-agent:8080` (targetPort 8000).

### Upstream Request Timeout

**Symptom:** `upstream request timeout` from Envoy

**Cause:** The LLM inference takes longer than the Envoy route timeout.

**Fix:** The `envoy-config` ConfigMap sets the route timeout to 300 seconds (5 min).
If you still hit timeouts, check that the ConfigMap was applied correctly:

```bash
kubectl get configmap envoy-config -n team1 -o jsonpath='{.data.envoy\.yaml}' | grep "timeout:"
```

If you see `30s` values instead of `300s`, re-apply the ConfigMaps and restart:

```bash
kubectl apply -f demos/github-issue/k8s/configmaps.yaml
kubectl rollout restart deployment/git-issue-agent -n team1
```

### Agent Pod Not Starting (4/4 containers)

**Symptom:** Pod shows 3/4 or less containers ready

**Fix:** Check each container's logs:

```bash
kubectl logs deployment/git-issue-agent -n team1 -c kagenti-client-registration
kubectl logs deployment/git-issue-agent -n team1 -c spiffe-helper
kubectl logs deployment/git-issue-agent -n team1 -c envoy-proxy
kubectl logs deployment/git-issue-agent -n team1 -c agent
```

### Tool MCP Server Unreachable / Connection Reset

**Symptom:** Agent returns `Couldn't connect to the MCP server after 60 seconds`, or
direct curl to the tool gets `Connection reset by peer`.

**Possible causes:**

1. **AuthBridge sidecars injected** — The webhook injected envoy-proxy into the tool
   pod, causing a port 9090 conflict. Check container count:
   ```bash
   kubectl get pods -n team1 | grep github-tool
   # If you see 3/3 instead of 1/1, sidecars were injected
   ```
   **Fix:** Follow the [Patch: Remove AuthBridge sidecars](#patch-remove-authbridge-sidecars-from-the-tool) section above.

2. **Service port mismatch** — Verify the tool service uses port 9090 (matching the agent's `MCP_URL`):
   ```bash
   kubectl get svc github-tool-mcp -n team1 -o jsonpath='{.spec.ports[0].port}:{.spec.ports[0].targetPort}'
   # Should show 9090:9090. If not, patch:
   kubectl patch svc github-tool-mcp -n team1 --type='json' \
     -p='[{"op":"replace","path":"/spec/ports/0/port","value":9090},{"op":"replace","path":"/spec/ports/0/targetPort","value":9090}]'
   ```

### GitHub Tool Returns 401

**Symptom:** Tool rejects the exchanged token

**Fix:** Verify the tool's environment variables match the Keycloak configuration:
- `ISSUER` should be `http://keycloak.localtest.me:8080/realms/demo`
- `AUDIENCE` should be `github-tool`

---

## Cleanup

### Via Kagenti UI

1. Go to the **Agent Catalog**, find `git-issue-agent`, and click **Delete**.
2. Go to the **Tool Catalog**, find `github-tool`, and click **Delete**.

### Via CLI

```bash
kubectl delete deployment git-issue-agent -n team1
kubectl delete deployment github-tool -n team1
kubectl delete svc git-issue-agent -n team1
kubectl delete svc github-tool-mcp -n team1
kubectl delete secret github-tool-secrets -n team1
kubectl delete pod test-client -n team1 --ignore-not-found
```

### Delete ConfigMaps

```bash
kubectl delete -f demos/github-issue/k8s/configmaps.yaml
```

### Delete Namespace (removes everything)

```bash
kubectl delete namespace team1
```

### Remove Webhook (optional)

```bash
kubectl delete mutatingwebhookconfiguration kagenti-webhook-authbridge-mutating-webhook-configuration
```

---

## Files Reference

| File | Description |
|------|-------------|
| `demos/github-issue/demo-ui.md` | This guide |
| `demos/github-issue/demo-manual.md` | Fully manual deployment guide |
| `demos/github-issue/setup_keycloak.py` | Keycloak configuration script |
| `demos/github-issue/k8s/configmaps.yaml` | ConfigMaps for AuthBridge sidecars |
| `demos/github-issue/k8s/git-issue-agent-deployment.yaml` | Agent deployment YAML (manual only) |
| `demos/github-issue/k8s/github-tool-deployment.yaml` | GitHub tool deployment YAML (manual only) |

## Next Steps

- **Manual Deployment**: See [demo-manual.md](demo-manual.md) for deploying everything via `kubectl`
- **AuthProxy Details**: See the [AuthProxy README](../../AuthProxy/README.md) for inbound
  JWT validation and outbound token exchange internals
- **Multi-Target Demo**: See the [multi-target demo](../multi-target/demo.md) for
  route-based token exchange to multiple tool services
- **Access Policies**: See the [access policies proposal](../../PROPOSAL-access-policies.md)
  for role-based delegation control
- **AuthBridge Overview**: See the [AuthBridge README](../../README.md) for architecture details
